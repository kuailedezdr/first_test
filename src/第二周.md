## 第一周

#### 1、spark的任务提交流程：

yarn-client模式提交流程

>1、客户端提交application，并且在当前客户端启动driver进程
>
>2、客户端发送请求带resourcemanager
>
>3、resourcemanager（rs）收到客户端的发来的请求，在集群中启动一个namenode（nm）创建applicationmaster（am）同时进行spark context的初始化：生成两个对象：dagschedule和taskschedule
>
>4、am启动后向rm请求资源，用于启动executor
>
>5、rm向am返回一批有资源的其他的namenode节点
>
>6、am使用某些namenode启动executor
>
>7、driver向executor发送task，并且监控执行情况和回收结果

yarn-cluster模式

> 1、客户端提交application，发送请求到resource manager（rm），请求启动application master（am）
>
> 2、rm收到请求后，在集群上随便找一台namenode节点，启动am
>
> 3、am启动后（相当于driver）发送请求到rm，请求资源，用于启动executor
>
> 4、rm接受请求，返回一批nm，用于启动executor
>
> 5、am连接返回来的nm启动executor
>
> 6、executor启动后，向am的driver进行注册
>
> 7、driver向exector发送task，并且监控executor的执行情况，和返回结果



2. #### 写出下列代码的打印结果。（5分）

def joinRdd(sc:SparkContext) {

​	val name= Array((1,"spark"),(2,"flink"),(3,"hadoop"),(4,”java”))

​	val score= Array((1,100),(2,90),(3,80),(5,90))

​	val namerdd=sc.parallelize(name);

​	val scorerdd=sc.parallelize(score);

​	val result = namerdd.join(scorerdd);

​	result.collect.foreach(println);

}

答案：

(2,(flink,90))
(1,(spark,100))
(3,(hadoop,80))



##### 3.写出触发shuffle的算子（至少五个）(5分)

reduceby 、 groupby 、 join 、  sortby 、distinct 、 repartition



##### 4.RDD的概念和特性（10分）

rdd一种抽象的数据类型，可以使用各种算子进行处理

特性：

是一个懒执行的不可变的可以支持function的并行数据集合

人性化程度很高

会自动的根据内存的境况缓存运算

由partition的概念，可以及进行排序和分片



5.Spark的这些参数什么意思 ？（5分）

--master 指定主节点

--class 主类名，应用程序的主类

--executor-memory  每个executor的内存

--total-executor-cores 所有executor的总核数

--num-executor 启动executor的数量



6. Transformtion算子和Action算子的区别（举例说明）(5分)

transformation：rdd的转变：从一个rdd--》rdd ：map（rdd）、groupby（rdd），生成的还是rdd

action：rdd到一个具体数据的转变：从一个rdd --》 数 ：rdd.count（）、rdd.first（）由rdd转别为一个数



7. 什么是持久化，为什么要用持久化？（5分）

将数据缓存到磁盘上或者内存上

避免发生意外数据导致数据丢失，spark的某些时候可以提高计算速度



8. 统计下面语句中，Spark的出现次数，哪个单词出现的次数最多。（10分）

Get Spark from the [downloads page](http://spark.apache.org/downloads.html) of the project website This documentation is for,Spark,version,Spark,uses,Hadoop,s,client,libraries,for,HDFS,and,YARN.Downloads are pre packaged.for a handful of popular.Hadoop versions Users can also download a Hadoop free binary and run Spark with any Hadoop version [by augmenting Spark s classpath](http://spark.apache.org/docs/latest/hadoop-provided.html) Scala and Java users can include.Spark in their projects using its Maven coordinates and in the future Python users can also install Spark from PyPI.

```scala
    val str_text = "Get Spark from the downloads page of the project website This documentation is for,Spark,version,Spark,uses,Hadoop,s,client,libraries,for,HDFS,and,YARN.Downloads are pre packaged.for a handful of popular.Hadoop versions Users can also download a Hadoop free binary and run Spark with any Hadoop version by augmenting Spark s classpath Scala and Java users can include.Spark in their projects using its Maven coordinates and in the future Python users can also install Spark from PyPI."
    val str = str_text.toLowerCase.replace(",", " ")
    val str1 = str.replace(".", " ")
    val lixi_one: SparkConf = new SparkConf().setMaster("local").setAppName("lixi_one")
    val context = new SparkContext(lixi_one)
    val strings:Array[String] = str1.split(" ")
    val value = context.makeRDD(strings).map((_, 1))
    .reduceByKey(_+_).sortBy(
        (value1) => {
            value1._2
        },false
    ).take(3)
    value.foreach(println)
```

```
(spark,7)
(hadoop,4)
(and,4)
```



9. 有如下数据

val list =

List((“jack”,100),(“jack”,80),(“tom”,88),(“jack”,56),(“tom”,66),(“tom”,78),(“tom”,99))

分组取Top2？(10分)

```scala
val lixi_one: SparkConf = new SparkConf().setMaster("local").setAppName("lixi_one")
val context = new SparkContext(lixi_one)

val list = List(("jack",100),("jack",80),("tom",88),("jack",56),("tom",66),("tom",78),("tom",99))

val value = context.makeRDD(list)
  .groupByKey().mapValues(
  (value)=>{
    value.toList.sortWith(
      (v1,v2)=>{
        v1.toInt>v2.toInt
      }
    ).take(2)
  }
)
value.foreach(println)
```

>(tom,List(99, 88))
>(jack,List(100, 80))



10. 谈谈你对广播变量的理解和它的使用场景（10分）	

**理解**：将driver端的变量发送给每一个任务executor，以便于executor的执行任务时使用。若有些task可能使用同一份变量（全局变量），这就会导致发送多次liat变量，，但是如果发送的次数多，或者list很大，会导致内存溢出，为了防止上面的情况，就使用了广播，将所有的变量一起发送给executor，task需要就从executor上的变量区去拿。但是也可能保存一些用不到的变量，浪费内存。

**使用场景**：executor执行任务时，多次shuffle操作，但是用到同一个变量。就是多次task用到了同一个变量，避免重复的接受变量保存浪费内存。



11. 检查点如何设置？使用检查点的好处？（5分）

某些rdd操作完后，执行rdd.checkpoint()

避免某些rdd的过长依赖发生意外，造成数据丢失或者错误，又要再次从新开始做，增加了容错性，减少了重复开销。



12. 累加器的作用？

在统计某些数据时，需要用到前面的数据，就需要将数据保存下来，相当于将生成的变量持久化，然后不断地更新累加器变量，达到数据累加的效果。



13. Task分为几种？说说Task的原理（10分）

两种：resultTask、shuffleMapTask

接收到RDD，将数据类型序列化，写入不同的分区，不断地进行数据的trasnformation操作，这就是shuffleMapTask过程，一直到需要将数据shuffle，则最后一次就是resultTask；



14. Spark和Hadoop的MR区别？（5分）

1、效率：spark是基于内存运算的，mr是磁盘并加以不断地持久化导致效率比spark的效率低一些

2、稳定性：但是spark使用的内存的稳定性不强，可能使得数据丢失或者宕机，mr的运算就要稳定了很多

3、运行的场所不同：mr是运行在hdfs上的，spark直接在本地计算



### 第二周

1. 1Spark中Worker的主要作用是什么？（5分）

主要功能：管理当前节点内存和CPU使用状况，接收master分配过来的资源指令，通过ExecutorRunning启动程序分配任务，Worker类似于包工头，管理分配新任务。做计算服务时，相当于process。

 

2. 有一个test.json文件，里面的数据如下：

{"age" : ”23” , "name":"xiaoming" }

利用sparkSQL方式读取文件，然后使用DataFrame进行显示，使两种方式创建DataFrame。（手写代码实现）（10分）

 ```scala
val options :mutable.Map[String,String] = mutable.Map[String,String]()
    csvOptions.+=(("sep",","))
    csvOptions.+=(("header","true"))
    csvOptions.+=(("nullValue","NULL"))
    csvOptions.+=(("emptyValue","Unkonw"))
    csvOptions.+=(("nanValue","nanValue"))
val conf = new SparkConf().setAppName("two_2").setMaster("local[4]")
val sparksession = new SparkSession.Builder().config(conf).getOrCreate()
import sparksession.implicits._
val leftSchema:StructType = StructType.fromDDL("name String,age Long,temp Long,ct Long")
val rightSchema:StructType = StructType.fromDDL("name String,salary Long")
sparksession.read.options(options).schema(leftShemae).json("file:///D:/test.json")
leftDf.createOrReplaceTempView("left")
leftDf.printSchema()
leftDf.show(10)
 ```

```scala
val conf = new SparkConf()
conf.setMaster("local[4]").setAppName("jsonrdd")
val sc = new SparkContext(conf)
val sqlContext = new SQLContext(sc)
 
val nameRDD = json("file:///D:/test.json")
val nameDF = sqlContext.read.json(nameRDD)
nameDF.registerTempTable("name")        
val result = sqlContext.sql("select * from name")
result.show()
sc.stop()
```





3. Driver的功能是什么？？（5分）

1、 运行应用程序的main函数

2、创建sparkde上下文

3、划分rdd的dag

4、与sparkd的其他组进行协调资源

5、生成task到executor



4. 解释一下什么窗口函数？使用具体的例子说明如何应用（需要数据，需求说明以及sql说明）？（10分）

 对指定区域内的数据进行实时处理



```scala
val conf = SparkHelper.createDefSparkConf(appName)
  .setMaster("local[3]")
val ssc = new StreamingContext(conf, Seconds(duration))

ssc.checkpoint(checkpoint)

val host = "localhost"
val port = 9000
val socketDS :ReceiverInputDStream[String] = ssc.socketTextStream(host, port)

val winDuration = duration * 2
val slideWinDuration = duration * 2

val ds = socketDS.map((_,CommonUtil.formatDate4Def(new Date()))).window(Seconds(winDuration))

ds.print()
```

5. 谈谈你知道的spark有哪些性能优化（并解释如何优化）（5分）

 参数优化：

1、根据网络速度和内存的写入和写出调整spark的时间间隔和缓冲片大小，避免push和pull的不均衡，导致数据异常；如果内存资源丰富：将spark.shuffle.file.buffer、spark.reducer.maxSizeInFlight、spark.shuffle.io.retryWait适当增大

2、 发生异常，导致spark重新连接数时，重试次数：spark.shuffle.io.maxRetries，若是，数据的产生比较困难，增大重试次数

3、shuffle read task的聚合的内存比例：若是，内存充足，并且很少持久化spark.shuffle.memoryFraction适当提高

数据倾斜优化：

1、etl预处理：不理想

2、提高shuffle的并行度：效果有限

3、reduce join 转map join：适用于大表和一个小表的情况

4、采样倾斜：拆分join操作

5、加盐：将某些字段编译和倾斜字段结合

代码调优：

1、设计好代码：多练习，熟练掌控各个算子的特点

2、减少shuffle的算子

3、可以使用检查点



6. Spark任务运行原理？（5分）

​        Spark的Application在运行时，首先在Driver程序中会创建SparkContext，将其作为调度的总入口，在其初始化的过程中会分别创建DAGSchedule（进行Stage调度）和TaskSchedule（进行Task调度）两个模块。DAGSchedule模块是基于Stage的调度模块，它为每个Spark job计算具有依赖关系的多个Stage任务阶段，然后将每个Stage划分为一组具体的任务（通常会考虑数据的本地性）以TaskSet的形式提交给底层的TaskSchedule模块来具体执行。TaskSchedule负责具体启动任务，监控和汇报任务的运行情况。而任务运行所需的资源需要向Cluster Manager申请。 



7. 谈谈spark中的宽窄依赖（5分）

 宽窄依赖：主要看被几个子RDD继承，若是只有一个儿子，那就是窄依赖，否者宽依赖；再者就是是否发生了shuffle，一般发生了shuffle就是宽依赖。

 

8. 谈谈你对kafka的理解，是否存在主从之分，为什么使用，说明原因，zookeeper在kafka中起到什么作用，详细说明。（5分）

有主从之分， 方便数据的传输，只需要给主节点发送数据就可以了，不会发生接收方内存不足的情况；有了固定的主节点，也不会发生数据备份找不到地方，直接从主节点备份就可以了；而且主节点的性能一般要强一些，副节点可以相对的差一点，也能节省资金。

帮助spark搭建高可用：

1、分布式协调合通知：心跳机制，保证节点的存活可用

2、master选举：



9. Kafka分区和消费者的关系？（5分）

 尽量满足一个消费者对应一个分区，或者一个消费者对应两个分区，如果分区多，消费者多忙一会，但是消费者多就会造成消费者处于空闲状态，浪费资源

 

10. 请说一下kafka为什么需要副本，原始分区和副本是如何设置的，当消费者进行消费数据的时候，怎么才可以精准的消费到数据？（5分）

 防止数据的丢失，保证数据的完善和可用。

一般一个分区一个topic，有两个副本

需要保证数据：1、数据有状态2、存储容器具有幂等性



11. 已知学生数据如下：请用spark core完成下列需求（25分）

![img](file:///C:\Users\49876\AppData\Local\Temp\ksohtml16960\wps1.jpg) 

```scala
case class student(stuid:String,name:String,birday:String,height:Int)
def main(args: Array[String]): Unit = {
  val lixi_one: SparkConf = new SparkConf().setMaster("local").setAppName("lixi_one")
  val context = new SparkContext(lixi_one)
  val value = context.textFile("file:///D:stu.txt")
    .map(line => {
      val Array(clas,stuid,sex,name,birday,abo,adress,height,phone) = line.split("\t")
      val name1 = student(stuid, name, birday, height.toInt)
      name1
    })
```

12.1按照身高排序（5分）

 ```scala
val values = value.sortBy((_.height),false)
values.foreach(println)	
 ```



12.2求平均年龄（5分）

 

12.3求学生中出现的所有姓氏（5分）

```scala
value.map(
    (va) => {
        (va.name.head,1)
    }
).groupByKey().count()
```

 

12.4返回出生在每月最大天数的3人（5分）

 ```scala
value.map(
    (va)=>{
        val Array(year,mon,day) = va.birday.split("-")
        (va.stuid,day)
    }
).sortBy(_._2,false)
 ```



12.5索引出相同生日下同学的姓名链表（5分）

 ```scala
value.map((va) => {
    val Array(year,mon,day) = va.birday.split("-")
    val monday=mon+day;
    (monday,va.name)
}).groupByKey()
 ```





13. 手动实现Kafka的API操作（15分）

13.1 编写一个kafka的生产者API，要求手动输入值就能够推送到kafka（5分）

```scala
public static void sendMsg(String path, String topic, Map<String,String> datas) throws Exception{
    Validate.notEmpty(path, "kafka config path is not empty");
    Validate.notEmpty(topic, "topic is not empty");
    Validate.notNull(datas, "datas is not empty");

    KafkaProducer producer = KafkaUtil.createProducer(path);
    if(null != producer){
        List<String> lines = new ArrayList<String>();
        for(Map.Entry<String, String> entry : datas.entrySet()){
            String key = entry.getKey();
            String value = entry.getValue();
            log.info("Producer.key=" + key + ",value=" + value);

            producer.send(new ProducerRecord<String, String>(topic, key, value));
            producer.flush();
        }
        producer.close();
    }

}
```



13.2 编写一个kafka的消费者API，要求循环接受消费消息（5分）

```scala
public static void seCom(String  topic) throws Exception{
    //配置消费者环境
    Properties props = new Properties();
    props.load(KafkaUtil.class.getClassLoader().getResourceAsStream(CommonConstant.KAFKA_CONSUMER_PATH));

    //构建消费者
    KafkaConsumer consumer = new KafkaConsumer<>(props);
    consumer.subscribe(Arrays.asList(topic));

    //遍历取出元素
    while(true){
        ConsumerRecords<String,String> consumerRecords = consumer.poll(Duration.ofSeconds(10L));
        System.out.println("进去吧");
        for(ConsumerRecord<String,String> consumerRecord: consumerRecords){
            String key = consumerRecord.key();
            String values = consumerRecord.value();
            Map mapvalues = GsonUtil.gObject2Json(values, Map.class);
            String temp = mapvalues.get("temp").toString();
            String topics = consumerRecord.topic();
            int partition = consumerRecord.partition();
            long offset = consumerRecord.offset();
            String str = "zuhe"+key+topics+partition+offset+"wan";
            System.out.println("消费者"+str);
        }
    }
}
```

13.3 编写一个kafak的API，能够创建删除修改主题（5分）





### 三周

1. Redis支持哪几种数据结构？（5分）

   String、hash、List、Set、SortedSet

2. Redis的持久化如何实现？（5分）
   通过快照完成，当符合一定条件时，redis会自动将内存中的数据进行快照并持久化到硬盘

3. Spark Streaming的UpdateStateByKey算子和mapWithState算子的区别？（5分）

   updatestatebykey:会统计所有的key，包括指定这一批时间内没有产生的key
   mapwithstate：只更新在这一段时间内出现的key

   

4. Kafka的文件存储机制？（5分）

   文件存储在kafka下的store的partition里面，包括文件的索引和文件，文件名一般是偏移量的大小，每当文件存储kafka设置的文件大小就生成下一个文件，以此类推。

   

5. Spark提交任务运行流程原理（写步骤不用画图）（10分）

   Spark的Application在运行时，首先在Driver程序中会创建SparkContext，将其作为调度的总入口，在其初始化的过程中会分别创建DAGSchedule（进行Stage调度）和TaskSchedule（进行Task调度）两个模块。DAGSchedule模块是基于Stage的调度模块，它为每个Spark job计算具有依赖关系的多个Stage任务阶段，然后将每个Stage划分为一组具体的任务（通常会考虑数据的本地性）以TaskSet的形式提交给底层的TaskSchedule模块来具体执行。TaskSchedule负责具体启动任务，监控和汇报任务的运行情况。而任务运行所需的资源需要向Cluster Manager申请。

   

6. 用编码的方式实现SparkSQL查询，要求：（10分）

1）获取HDFS的数据

2）用StructType的方式生成Schema

3）生成DataFrame

4）生成临时表

5）结果输出到HDFS

6）代码写到背面val conf = new SparkConf().setMaster("local").setAppName("")

```scala
    val spark = SparkSession.builder().config(conf).getOrCreate()

    import spark.implicits._

    val input = "file:///D:/data/employee.json"

    val ql = spark.read.json(input)

    ql.createOrReplaceTempView("user")

    val sql1 = spark.sql("select max(salary) max_s,gender, region_code from user group by grouping sets ((region_code,gender),(region_code))")

    sql1.createOrReplaceTempView("user1")
    val sql2 = spark.sql("select max(max_s) s from user1 group by region_code,gender  with rollup ")
    .orderBy(desc("max_s")).limit(2).show()
```
7. GC的原理？（10分）
   垃圾回收机制：

   伊甸园：对象最初诞生的区域

   新生代：幸存者活过的区域：采用多线程和复制算法来进行垃圾回收 

   老年代：存活的最久的对象生活区，单线程和标记-整理算法来实现垃圾回收 

   

8. 谈谈foreachRDD和foreachPartition和foreach的理解和区别？（10分）

   foreachRDD、foreachPartition和foreach的不同之处主要在于它们的作用范围不同，foreachRDD作用于DStream中每一个时间间隔的RDD，foreachPartition作用于每一个时间间隔的RDD中的每一个partition，foreach作用于每一个时间间隔的RDD中的每一个元素。

   Foreach与ForeachPartition都是在每个partition中对iterator进行操作,不同的是,foreach是直接在每个partition中直接对iterator执行foreach操作,而传入的function只是在foreach内部使用,而foreachPartition是在每个partition中把iterator给传入的function,让function自己对iterator进行处理（可以避免内存溢出）

   

9. 数据形式：（10分）
aa 11
bb 11
cc 34
aa 22
需求：
1、对上述数据按key值进行分组
2、对分组后的值进行排序
3、分组后取top 3  以key-value形式返回结果

```scala
val list = List(("aa",11),("bb",11),("cc",34),("aa",22)
val maps = list.groupBy(_._1)
val val = maps.map( t => {
   val key = t._1
   val values = t._2.toList.sorted.reverse.take(3)
   (key,values)
})
val.foreach(println)
```



10. Spark On Yarn运行原理（Cluster模式）？（10分）
    1、客户端提交任务，向ResourceManager通讯申请启动ApplicationMaster,随后ResourceManager分配container，在合适的 NodeManager上启动ApplicationMaster,此时的appmaster就是driver

    2、driver向ResourceManager申请Executor 内存

    3、resourcemanager（rs）收到客户端的发来的请求，在合适的nodemanager上启动applicationmaster

    4、appmaster启动后向rm请求资源，用于启动executor

    5、rm收到appmaster的资源申请后会分配container,appmaster在资源分配指定的nodemanager上启动executor

    6、executor启动后向driver反向注册.driver向executor发送task，并且监控执行情况和回收结果

11. Spark Streaming的Receiver方式和直连方式的区别？（10分）
    Spark-Streaming获取kafka数据的两种方式-Receiver与Direct的方式，可以从代码中简单理解成Receiver方式是通过zookeeper来连接kafka队列，Direct方式是直接连接到kafka的节点上获取数据了。

12. 你如何理解scala中的trait的？（5分）


    一.java中的接口不是面向对象的只是对多继承的一种补充 而scala是纯面向对象的所以使用trait(特征)相当于java中interface+abstract class

    二.scala的没有implements关键字,它使用extends关键字实现trait

    三.scala沿用也java的库所以scala中java的所有库可以当做trait来使用

    四.scala也是单继承的使用trait满足了多继承的要求

    五.trait 可以拥有抽象方法也可以拥有实现的方法(我们将scala的class文件反编译成java文件了解底层原理)

    

    

13. 你如何理解scala中的伴生对象和伴生类的（5分）

    scala中class创建的是伴生类，object创建的是伴生对象

    伴生类中可以声明无参构造器和有参构造器，辅助构造器，属性，方法

    伴生对象属于单例对象，也可以声明属性，方法，但是不可以声明构造器

    scala创建对象可以通过new的方式也可以通过伴生对象的方式进行创建。但是如果想用伴生对象的方式进行创建就必须定义apply方法，在apply方法中通过new的方式创建对象。

    

写的不好，多多见谅
欲知下节如何，清关注一波

可以参考另外一份markdown，共同操作



